{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Detailed Code",
   "id": "a04e6353534bbacd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T15:02:11.915109Z",
     "start_time": "2025-11-18T14:56:17.473982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap\n",
    "\n",
    "def preprocess_data(df):\n",
    "    print(\"Initial data shape:\", df.shape)\n",
    "    print(\"Initial columns:\", df.columns.tolist())\n",
    "\n",
    "    # --- Name filtering ---\n",
    "    if 'Name' in df.columns:\n",
    "        df['Name'] = df['Name'].replace('AHU1.1', 'AHU.1')\n",
    "        df = df[df['Name'].str.contains('HVAC', na=False)]\n",
    "        print(f\"Shape after filtering 'HVAC' names: {df.shape}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'Name' column not found, skipping name-based filtering.\")\n",
    "\n",
    "    # --- Target variable check ---\n",
    "    if 'Active_Energy_Delivered' not in df.columns:\n",
    "        raise ValueError(\"Target column 'Active_Energy_Delivered' not found.\")\n",
    "    df['Active_Energy_Delivered'] = pd.to_numeric(df['Active_Energy_Delivered'], errors='coerce')\n",
    "    df.dropna(subset=['Active_Energy_Delivered'], inplace=True)\n",
    "    df = df[(df['Active_Energy_Delivered'] >= 0) & (df['Active_Energy_Delivered'] < 120)]\n",
    "    print(f\"Shape after filtering target: {df.shape}\")\n",
    "\n",
    "    # --- Date Handling ---\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        invalid = df['Date'].isna().sum()\n",
    "        df.dropna(subset=['Date'], inplace=True)\n",
    "        print(f\"Dropped {invalid} rows due to invalid 'Date'.\")\n",
    "        df = df.sort_values(by='Date')\n",
    "    else:\n",
    "        raise ValueError(\"Column 'Date' not found in dataset.\")\n",
    "\n",
    "    # --- Jumbo Temperature Imputation ---\n",
    "    temp_cols_exist = all(c in df.columns for c in ['Jumbo_Temp1', 'Jumbo_Temp2', 'Jumbo_Temp3'])\n",
    "    if temp_cols_exist:\n",
    "        for col in ['Jumbo_Temp1', 'Jumbo_Temp2', 'Jumbo_Temp3']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df['Jumbo_Temp1'] = df['Jumbo_Temp1'].fillna(df['Jumbo_Temp2']).fillna(df['Jumbo_Temp3'])\n",
    "        df.drop(columns=['Jumbo_Temp2', 'Jumbo_Temp3'], inplace=True)\n",
    "        df['Jumbo_Temp1'] = df['Jumbo_Temp1'].ffill().bfill()\n",
    "        print(\"‚úÖ Imputed Jumbo_Temp1\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Missing Jumbo_Temp columns, skipped imputation.\")\n",
    "\n",
    "    # --- Jumbo Humidity Imputation ---\n",
    "    humidity_cols_exist = all(c in df.columns for c in ['Jumbo_Humidity', 'Jumbo_Humidity3'])\n",
    "    if humidity_cols_exist:\n",
    "        for col in ['Jumbo_Humidity', 'Jumbo_Humidity3']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df['Jumbo_Humidity'] = df['Jumbo_Humidity'].fillna(df['Jumbo_Humidity3'])\n",
    "        df.drop(columns=['Jumbo_Humidity3'], inplace=True)\n",
    "        df['Jumbo_Humidity'] = df['Jumbo_Humidity'].ffill().bfill()\n",
    "        print(\"‚úÖ Imputed Jumbo_Humidity\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Missing Jumbo_Humidity columns, skipped imputation.\")\n",
    "\n",
    "    # --- Feature Engineering ---\n",
    "    if {'Avg_Return_Water_Temp', 'Avg_Supply_water_Temp'} <= set(df.columns):\n",
    "        df['Compressor_delta'] = (\n",
    "            pd.to_numeric(df['Avg_Return_Water_Temp'], errors='coerce') -\n",
    "            pd.to_numeric(df['Avg_Supply_water_Temp'], errors='coerce')\n",
    "        )\n",
    "        print(\"‚úÖ Created 'Compressor_delta'\")\n",
    "\n",
    "    # --- Range Filters ---\n",
    "    if 'T2M' in df.columns:\n",
    "        df = df[(df['T2M'] > 0) & (df['T2M'] < 50)]\n",
    "    if 'Operating_Hours' in df.columns:\n",
    "        df = df[(df['Operating_Hours'] > 0) & (df['Operating_Hours'] < 1.1)]\n",
    "\n",
    "    # --- Final Fill ---\n",
    "    df = df.ffill().bfill()\n",
    "\n",
    "    print(\"‚úÖ Preprocessing complete.\")\n",
    "    print(f\"Final cleaned data shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_prepare(path):\n",
    "    # Auto-detect Excel or CSV\n",
    "    if path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "        df = pd.read_excel(path)\n",
    "        df=df.fillna(0)\n",
    "    else:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", encoding_errors=\"ignore\", parse_dates=[\"Date\"])\n",
    "    df = preprocess_data(df)\n",
    "    # Encode Machine_Name (RandomForest cannot take text)\n",
    "    le = LabelEncoder()\n",
    "    df[\"Machine_Encoded\"] = le.fit_transform(df[\"Name\"])\n",
    "\n",
    "    # Independent variables\n",
    "    features = [\n",
    "        \"Machine_Encoded\", \"T2M\", \"Operating_Hours\", \"Jumbo_Temp1\", \"Jumbo_Humidity\",\n",
    "        \"Average_Voltage_Line_to_Line\", \"Average_Voltage_Line_to_Neutral\",\n",
    "        \"Avg_Supply_water_Temp\", \"Avg_Return_Water_Temp\", \"Compressor_delta\",\n",
    "        \"1st_Shift\", \"2nd_Shift\", \"common\", \"General\", \"hour\", \"Month\", \"Day\"\n",
    "    ]\n",
    "\n",
    "    return df, features, le\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2Ô∏è‚É£ Split Train/Test by Date\n",
    "# ---------------------------------------------\n",
    "def split_by_date(df):\n",
    "    latest_date = df[\"Date\"].max()\n",
    "    test_start = latest_date - pd.Timedelta(days=30)\n",
    "    train = df[df[\"Date\"] < test_start]\n",
    "    test = df[df[\"Date\"] >= test_start]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3Ô∏è‚É£ Evaluate Model\n",
    "# ---------------------------------------------\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"Train\": {\n",
    "            \"MSE\": mse_train,\n",
    "            \"RMSE\": rmse_train,\n",
    "            \"R2\": r2_score(y_train, y_pred_train),\n",
    "        },\n",
    "        \"Test\": {\n",
    "            \"MSE\": mse_test,\n",
    "            \"RMSE\": rmse_test,\n",
    "            \"R2\": r2_score(y_test, y_pred_test),\n",
    "        },\n",
    "        \"OOB\": {\n",
    "            \"R2\": getattr(model, \"oob_score_\", None)\n",
    "        }\n",
    "    }\n",
    "    return metrics, y_pred_train, y_pred_test\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4Ô∏è‚É£ SHAP: Top N Features per Prediction\n",
    "# ---------------------------------------------\n",
    "def shap_feature_importance(model, X, top_n=5):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Extract top N impactful features for each row\n",
    "    top_features = []\n",
    "    for row in np.abs(shap_values):\n",
    "        top_idx = np.argsort(row)[::-1][:top_n]\n",
    "        top_feat_names = [feature_names[i] for i in top_idx]\n",
    "        top_features.append(top_feat_names)\n",
    "\n",
    "    top_features_df = pd.DataFrame(top_features, columns=[f\"Top_Feature_{i+1}\" for i in range(top_n)])\n",
    "    return top_features_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5Ô∏è‚É£ Complete Pipeline\n",
    "# ---------------------------------------------\n",
    "def hvac_pipeline(data_path):\n",
    "    # Load and prepare\n",
    "    df, features, le = load_and_prepare(data_path)\n",
    "    train, test = split_by_date(df)\n",
    "\n",
    "    X_train, y_train = train[features], train[\"Active_Energy_Delivered\"]\n",
    "    X_test, y_test = test[features], test[\"Active_Energy_Delivered\"]\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=80,\n",
    "        #max_depth=12,\n",
    "       # min_samples_split=5,\n",
    "        # min_samples_leaf=10,\n",
    "       # max_features='sqrt',\n",
    "        oob_score=True,\n",
    "        # random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics, y_pred_train, y_pred_test = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # SHAP explainability\n",
    "    top_features_df = shap_feature_importance(model, X_test, top_n=5)\n",
    "\n",
    "    # Merge all data with predictions\n",
    "    test_result = test.copy()\n",
    "    test_result[\"Predicted_Power\"] = y_pred_test\n",
    "    test_result[\"Error_%\"] = ((test_result[\"Predicted_Power\"] - test_result[\"Active_Energy_Delivered\"])\n",
    "                              / test_result[\"Active_Energy_Delivered\"]) * 100\n",
    "\n",
    "    # Add anomaly tag (spike detection)\n",
    "    test_result[\"Anomaly_Flag\"] = np.where(\n",
    "        np.abs(test_result[\"Error_%\"]) > 20,\n",
    "        np.where(test_result[\"Error_%\"] > 0, \"Overconsumption\", \"Underconsumption\"),\n",
    "        \"Normal\"\n",
    "    )\n",
    "\n",
    "    # Append top features (SHAP)\n",
    "    test_result = pd.concat([test_result.reset_index(drop=True), top_features_df], axis=1)\n",
    "\n",
    "    # Save final combined results\n",
    "    output_path = \"D:/Study/Projects/TATA_TOP_AND_HVAC_Projects/HVAC_Project/test_predictions_with_shap.xlsx\"\n",
    "    test_result.to_excel(output_path, index=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Test predictions with SHAP and full columns saved to:\")\n",
    "    print(output_path)\n",
    "\n",
    "    return metrics, test_result\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6Ô∏è‚É£ Run the Pipeline\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = \"D:/Study/Projects/TATA_TOP_AND_HVAC_Projects/HVAC_Project/HVAC_data.xlsx\"\n",
    "    metrics, test_result = hvac_pipeline(data_path)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Metrics:\")\n",
    "    print(pd.DataFrame(metrics).T)"
   ],
   "id": "219aa7c95d692ea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (317845, 32)\n",
      "Initial columns: ['Time-Hourly', 'Name', 'Active_Energy_Delivered', 'Operating_Hours', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Humidity3', 'Jumbo_Temp2', 'Jumbo_Temp1', 'Jumbo_Temp3', 'Jumbo_Humidity', 'T2M', 'Date_Time', 'Avg_Return_Water_Temp', 'Avg_Supply_water_Temp', 'TimeStamp', 'Year', 'Month', 'Day', 'hour', 'Date', 'Start Date', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Left_Right_Name', 'Score', 'Right_Date_Time', 'Right_3rd_Shift', 'Result']\n",
      "Shape after filtering 'HVAC' names: (109750, 32)\n",
      "Shape after filtering target: (98663, 32)\n",
      "Dropped 0 rows due to invalid 'Date'.\n",
      "‚úÖ Imputed Jumbo_Temp1\n",
      "‚úÖ Imputed Jumbo_Humidity\n",
      "‚úÖ Created 'Compressor_delta'\n",
      "‚úÖ Preprocessing complete.\n",
      "Final cleaned data shape: (19011, 30)\n",
      "\n",
      "‚úÖ Test predictions with SHAP and full columns saved to:\n",
      "D:/Study/Projects/TATA_TOP_AND_HVAC_Projects/HVAC_Project/test_predictions_with_shap.xlsx\n",
      "\n",
      "üìä Evaluation Metrics:\n",
      "             MSE      RMSE        R2\n",
      "Train   3.637349  1.907184  0.990916\n",
      "Test   97.094750  9.853667  0.903862\n",
      "OOB          NaN       NaN  0.932516\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All testing",
   "id": "f967b50c30c3cb33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8fbc018b28cb5b3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7401002215dfe540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8509d3ad5fc38726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c42854abae847903"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "186cb02a7a16bafa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71bc557d44a67521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d93151b3705db195"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7d3290fd2702438d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae479fbd1b58a57b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ac5b9cc5f2f7df6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "612f7bbaee7dfce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3455dd16a58a18d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "26146dacff470b18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1c1bf3bd8d6e519"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b9caf9222a693a03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d81def4a8adfdd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5403c8c8b2d0f7f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24d8c2d3a7b7c336"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8adbd6b0280b8314"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a01a2e474ddd4901"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "439976f18bfbfe69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d65b341f32f3a30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ab50f14bc3b995c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "90748157f1ef3bdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b6c8ed3176b1321"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "17240053eec3488c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "952ea0ceec4fec37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "760fb0e593e2886e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80b1bac3136985d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86d4e1b9ec77955b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6664508f62e2fddb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "662c7d8ee45a4603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e3f15ed1629628eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3f3cb9c90d96f21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25aedf2de8820b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65c94fb5e2bda4fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "91b9f66ed05e00c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad361d6f19d62ab2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f091c86a23721ac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d5c9f6556e049273"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d1343f97f898888"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d767968fa4751360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c0da29d82031f90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b361775358ad8d77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Old Code",
   "id": "16aeb98866862b42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1Ô∏è‚É£ Load & Prepare Data\n",
    "# ---------------------------------------------\n",
    "def load_and_prepare(path):\n",
    "    # Auto-detect Excel or CSV\n",
    "    if path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "        df = pd.read_excel(path)\n",
    "        df=df.fillna(0)\n",
    "    else:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", encoding_errors=\"ignore\", parse_dates=[\"Date\"])\n",
    "\n",
    "    # Encode Machine_Name (RandomForest cannot take text)\n",
    "    le = LabelEncoder()\n",
    "    df[\"Machine_Encoded\"] = le.fit_transform(df[\"Name\"])\n",
    "\n",
    "    # Independent variables\n",
    "    features = [\n",
    "        \"Machine_Encoded\", \"T2M\", \"Operating_Hours\", \"Jumbo_Temp1\", \"Jumbo_Humidity\",\n",
    "        \"Average_Voltage_Line_to_Line\", \"Average_Voltage_Line_to_Neutral\",\n",
    "        \"Avg_Supply_water_Temp\", \"Avg_Return_Water_Temp\", \"Compressor_delta\",\n",
    "        \"1st_Shift\", \"2nd_Shift\", \"common\", \"General\", \"hour\", \"Month\", \"Day\"\n",
    "    ]\n",
    "\n",
    "    return df, features, le\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2Ô∏è‚É£ Split Train/Test by Date\n",
    "# ---------------------------------------------\n",
    "def split_by_date(df):\n",
    "    latest_date = df[\"Date\"].max()\n",
    "    test_start = latest_date - pd.Timedelta(days=30)\n",
    "    train = df[df[\"Date\"] < test_start]\n",
    "    test = df[df[\"Date\"] >= test_start]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3Ô∏è‚É£ Evaluate Model\n",
    "# ---------------------------------------------\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"Train\": {\n",
    "            \"MSE\": mse_train,\n",
    "            \"RMSE\": rmse_train,\n",
    "            \"R2\": r2_score(y_train, y_pred_train),\n",
    "        },\n",
    "        \"Test\": {\n",
    "            \"MSE\": mse_test,\n",
    "            \"RMSE\": rmse_test,\n",
    "            \"R2\": r2_score(y_test, y_pred_test),\n",
    "        },\n",
    "        \"OOB\": {\n",
    "            \"R2\": getattr(model, \"oob_score_\", None)\n",
    "        }\n",
    "    }\n",
    "    return metrics, y_pred_train, y_pred_test\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4Ô∏è‚É£ SHAP: Top N Features per Prediction\n",
    "# ---------------------------------------------\n",
    "def shap_feature_importance(model, X, top_n=5):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Extract top N impactful features for each row\n",
    "    top_features = []\n",
    "    for row in np.abs(shap_values):\n",
    "        top_idx = np.argsort(row)[::-1][:top_n]\n",
    "        top_feat_names = [feature_names[i] for i in top_idx]\n",
    "        top_features.append(top_feat_names)\n",
    "\n",
    "    top_features_df = pd.DataFrame(top_features, columns=[f\"Top_Feature_{i+1}\" for i in range(top_n)])\n",
    "    return top_features_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5Ô∏è‚É£ Complete Pipeline\n",
    "# ---------------------------------------------\n",
    "def hvac_pipeline(data_path):\n",
    "    # Load and prepare\n",
    "    df, features, le = load_and_prepare(data_path)\n",
    "    train, test = split_by_date(df)\n",
    "\n",
    "    X_train, y_train = train[features], train[\"Active_Energy_Delivered\"]\n",
    "    X_test, y_test = test[features], test[\"Active_Energy_Delivered\"]\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        #max_depth=12,\n",
    "       # min_samples_split=5,\n",
    "        min_samples_leaf=10,\n",
    "       # max_features='sqrt',\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics, y_pred_train, y_pred_test = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # SHAP explainability\n",
    "    top_features_df = shap_feature_importance(model, X_test, top_n=5)\n",
    "\n",
    "    # Merge all data with predictions\n",
    "    test_result = test.copy()\n",
    "    test_result[\"Predicted_Power\"] = y_pred_test\n",
    "    test_result[\"Error_%\"] = ((test_result[\"Predicted_Power\"] - test_result[\"Active_Energy_Delivered\"])\n",
    "                              / test_result[\"Active_Energy_Delivered\"]) * 100\n",
    "\n",
    "    # Add anomaly tag (spike detection)\n",
    "    test_result[\"Anomaly_Flag\"] = np.where(\n",
    "        np.abs(test_result[\"Error_%\"]) > 20,\n",
    "        np.where(test_result[\"Error_%\"] > 0, \"Overconsumption\", \"Underconsumption\"),\n",
    "        \"Normal\"\n",
    "    )\n",
    "\n",
    "    # Append top features (SHAP)\n",
    "    test_result = pd.concat([test_result.reset_index(drop=True), top_features_df], axis=1)\n",
    "\n",
    "    # Save final combined results\n",
    "    output_path = \"D:/Study/Projects/TATA_TOP_AND_HVAC_Projects/HVAC_Project/test_predictions_with_shap.xlsx\"\n",
    "    test_result.to_excel(output_path, index=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Test predictions with SHAP and full columns saved to:\")\n",
    "    print(output_path)\n",
    "\n",
    "    return metrics, test_result\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6Ô∏è‚É£ Run the Pipeline\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = \"D:/Study/Projects/TATA_TOP_AND_HVAC_Projects/HVAC_Project/preprocessed_data.xlsx\"\n",
    "    metrics, test_result = hvac_pipeline(data_path)\n",
    "\n",
    "    print(\"\\nüìä Evaluation Metrics:\")\n",
    "    print(pd.DataFrame(metrics).T)\n"
   ],
   "id": "b7db5768d9136848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(\"Initial data shape:\", df.shape)\n",
    "# print(\"Initial columns:\", df.columns.tolist())\n",
    "# print(\"Initial data types:\\n\", df.dtypes)\n",
    "# # --- Data Preprocessing ---\n",
    "# print(\"\\nStarting Data Preprocessing...\")\n",
    "# print(\"First, we will look on HVAC(Chiller)\")\n",
    "# # Rename inconsistent names\n",
    "# if 'Name' in df.columns:\n",
    "#     df['Name'] = df['Name'].replace('AHU1.1', 'AHU.1')\n",
    "#     # Filter for relevant names (Still useful to filter rows, even if 'Name' isn't used as a feature)\n",
    "#     data = df[df['Name'].str.contains('HVAC', na=False)]\n",
    "#     print(f\"Shape after filtering Name: {data.shape}\")\n",
    "# else:\n",
    "#     print(\"Warning: 'Name' column not found for filtering.\")\n",
    "#\n",
    "#\n",
    "# # Filter target variable range\n",
    "# if 'Active_Energy_Delivered' in df.columns:\n",
    "#     # Ensure target is numeric before filtering\n",
    "#     df['Active_Energy_Delivered'] = pd.to_numeric(\n",
    "#         df['Active_Energy_Delivered'], errors='coerce')\n",
    "#     # Drop rows where conversion failed\n",
    "#     df.dropna(subset=['Active_Energy_Delivered'], inplace=True)\n",
    "#     df = df[(df['Active_Energy_Delivered'] >= 0) &\n",
    "#                 (df['Active_Energy_Delivered'] < 120)]  # Adjust upper limit if needed\n",
    "#     print(f\"Shape after filtering Active_Energy_Delivered: {df.shape}\")\n",
    "# else:\n",
    "#     print(\"Error: Target column 'Active_Energy_Delivered' not found. Exiting.\")\n",
    "#     exit()\n",
    "#\n",
    "# # Convert Date and handle errors\n",
    "# if 'Date' in df.columns:\n",
    "#     df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "#     initial_rows = df.shape[0]\n",
    "#     df = df.dropna(subset=['Date'])\n",
    "#     print(f\"Dropped {initial_rows - df.shape[0]} rows due to invalid Date format.\")\n",
    "#     print(f\"Shape after handling Date: {df.shape}\")\n",
    "#     df = df.sort_values(by='Date')\n",
    "# else:\n",
    "#     print(\"Error: 'Date' column not found for train/test split. Exiting.\")\n",
    "#     exit()\n",
    "#\n",
    "# temp_cols_exist = all(col in df.columns for col in ['Jumbo_Temp1', 'Jumbo_Temp2', 'Jumbo_Temp3'])\n",
    "# humidity_cols_exist = all(col in df.columns for col in ['Jumbo_Humidity', 'Jumbo_Humidity3'])\n",
    "#\n",
    "# if temp_cols_exist:\n",
    "#     print(\"Imputing Jumbo Temperatures...\")\n",
    "#     # Convert to numeric first\n",
    "#     for col in ['Jumbo_Temp1', 'Jumbo_Temp2', 'Jumbo_Temp3']:\n",
    "#         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "#     df['Jumbo_Temp1'] = df['Jumbo_Temp1'].fillna(df['Jumbo_Temp2']).fillna(df['Jumbo_Temp3'])\n",
    "# else:\n",
    "#     print(\"Warning: One or more Jumbo Temperature columns missing, skipping imputation.\")\n",
    "#\n",
    "# if humidity_cols_exist:\n",
    "#     print(\"Imputing Jumbo Humidity...\")\n",
    "#     # Convert to numeric first\n",
    "#     for col in ['Jumbo_Humidity', 'Jumbo_Humidity3']:\n",
    "#         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "#     df['Jumbo_Humidity'] = df['Jumbo_Humidity'].fillna(df['Jumbo_Humidity3'])\n",
    "# else:\n",
    "#     print(\"Warning: One or more Jumbo Humidity columns missing, skipping imputation.\")\n",
    "#\n",
    "# # Drop redundant columns if they exist AND imputation was attempted\n",
    "# cols_to_drop = []\n",
    "# if temp_cols_exist:\n",
    "#     cols_to_drop.extend(['Jumbo_Temp2', 'Jumbo_Temp3'])\n",
    "# if humidity_cols_exist:\n",
    "#     cols_to_drop.append('Jumbo_Humidity3')\n",
    "#\n",
    "# df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "# print(f\"Dropped redundant columns: {[col for col in cols_to_drop if col in df.columns]}\")\n",
    "#\n",
    "#\n",
    "# # Forward/Backward fill remaining NaNs for imputed columns\n",
    "# if 'Jumbo_Temp1' in df.columns:\n",
    "#     initial_nan = df['Jumbo_Temp1'].isna().sum()\n",
    "#     data['Jumbo_Temp1'] = df['Jumbo_Temp1'].ffill().bfill()\n",
    "#     print(f\"Filled {initial_nan - data['Jumbo_Temp1'].isna().sum()} NaNs in Jumbo_Temp1.\")\n",
    "# if 'Jumbo_Humidity' in df.columns:\n",
    "#     initial_nan = df['Jumbo_Humidity'].isna().sum()\n",
    "#     df['Jumbo_Humidity'] = df['Jumbo_Humidity'].ffill().bfill()\n",
    "#     print(f\"Filled {initial_nan - data['Jumbo_Humidity'].isna().sum()} NaNs in Jumbo_Humidity.\")\n",
    "#\n",
    "# # Filter based on T2M and Operating_Hours (Check column existence and convert to numeric)\n",
    "# if 'T2M' in df.columns:\n",
    "#    df = df[(df['T2M'] > 0) & (df['T2M'] < 50)]\n",
    "#    print(f\"Shape after filtering T2M: {df.shape}\")\n",
    "# if 'Operating_Hours' in df.columns:\n",
    "#    df = df[(df['Operating_Hours'] > 0) &\n",
    "#                (df['Operating_Hours'] < 1.1)]\n",
    "#    print(f\"Shape after filtering Operating_Hours: {df.shape}\")\n",
    "# else:\n",
    "#     print(\"Warning: 'Operating_Hours' column not found for filtering.\")\n",
    "#\n",
    "# if 'Avg_Return_Water_Temp' in df.columns and 'Avg_Supply_water_Temp' in df.columns:\n",
    "#    # Ensure columns are numeric before subtraction\n",
    "#    df['Avg_Return_Water_Temp'] = pd.to_numeric(df['Avg_Return_Water_Temp'], errors='coerce')\n",
    "#    df['Avg_Supply_water_Temp'] = pd.to_numeric(df['Avg_Supply_water_Temp'], errors='coerce')\n",
    "#    df['Compressor_delta'] = df['Avg_Return_Water_Temp'] - df['Avg_Supply_water_Temp']\n",
    "#    print(\"Compressor_delta created.\")\n",
    "# df.isnull().sum()\n",
    "# final_columns = [\n",
    "#    'Name', 'T2M', 'Operating_Hours', 'Active_Energy_Delivered','Jumbo_Temp1','Jumbo_Humidity','Average_Voltage_Line_to_Line',\n",
    "#    'Year', 'Month', 'Day', 'Date', 'hour', 'Avg_Supply_water_Temp','Current_Phase_Average', 'Average_Voltage_Line_to_Neutral',\n",
    "#    'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', 'common', 'General'\n",
    "# ]\n"
   ],
   "id": "7176f2ac8ee2733b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e8ea85a4b7bfc5c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b1ffa28fbf2bc804"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b710425ebbb6b187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d015b438120336cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
