{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T13:26:51.927360Z",
     "start_time": "2025-10-26T13:26:14.223155Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyodbc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder # No longer needed for this approach\n",
    "import shap  # Make sure SHAP is installed: pip install shap\n",
    "import traceback  # For detailed error printing\n",
    "import warnings  # To suppress potential warnings if desired\n",
    "\n",
    "# Optional: Suppress SHAP UserWarnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='shap')\n",
    "\n",
    "# # --- Database Connection Details ---\n",
    "# # !!! IMPORTANT: Replace with your actual credentials or use a secure method !!!\n",
    "# # Consider environment variables or a config file instead of hardcoding\n",
    "# server_name = '10.10.9.32'\n",
    "# database_name = 'TLMAL'\n",
    "# user_id = 'taslview'\n",
    "# password = 'New@1234'  # This is insecure - manage secrets properly!\n",
    "# # Ensure this driver is installed and matches your system (e.g., '{ODBC Driver 17 for SQL Server}')\n",
    "# driver = '{SQL Server}'\n",
    "#\n",
    "# # --- Database Connection ---\n",
    "# try:\n",
    "#     # F-string requires Python 3.6+\n",
    "#     connection_string = f\"Driver={driver};Server={server_name};Database={database_name};UID={user_id};PWD={password}\"\n",
    "#     conn = pyodbc.connect(connection_string)\n",
    "#     print(\"Database connection successful.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Database connection failed: {e}\")\n",
    "#     print(\"Please ensure the SQL Server is running, credentials are correct,\")\n",
    "#     print(\"the specified ODBC driver is installed, and network connectivity is okay.\")\n",
    "#     # Exit or handle error appropriately if connection fails\n",
    "#     exit()\n",
    "#\n",
    "# # --- SQL Query to Fetch Data ---\n",
    "# Power = '''SELECT [Time-Hourly]\n",
    "#       ,[Name]\n",
    "#       ,[Active_Energy_Delivered]\n",
    "#       ,[Operating_Hours]\n",
    "#       ,[Average_Voltage_Line_to_Neutral]\n",
    "#       ,[Average_Voltage_Line_to_Line]\n",
    "#       ,[Jumbo_Humidity3]\n",
    "#       ,[Jumbo_Temp2]\n",
    "#       ,[Jumbo_Temp1]\n",
    "#       ,[Jumbo_Temp3]\n",
    "#       ,[Jumbo_Humidity]\n",
    "#       ,[T2M]\n",
    "#       ,[Date_Time]\n",
    "#       ,[Avg_Return_Water_Temp]\n",
    "#       ,[Avg_Supply_water_Temp]\n",
    "#       ,[TimeStamp]\n",
    "#       ,[Year]\n",
    "#       ,[Month]\n",
    "#       ,[Day]\n",
    "#       ,[hour]\n",
    "#       ,[Date]\n",
    "#       ,[Start Date]\n",
    "#       ,[1st_Shift]\n",
    "#       ,[2nd_Shift]\n",
    "#       ,[3rd_Shift]\n",
    "#       ,[common]\n",
    "#       ,[General]\n",
    "#       ,[Left_Right_Name]\n",
    "#       ,[Score]\n",
    "#       ,[Right_Date_Time]\n",
    "#       ,[Right_3rd_Shift]\n",
    "#       ,[Result]\n",
    "#   FROM [TLMAL].[dbo].[HVAC_Regression]'''  # Consider adding WHERE clauses here if possible to reduce data fetched\n",
    "#\n",
    "# try:\n",
    "#     print(\"Fetching data from database...\")\n",
    "#     data = pd.read_sql(Power, conn)\n",
    "#     conn.close()  # Close connection immediately after fetching data\n",
    "#     print(\"Data fetched successfully.\")\n",
    "#     # Optional: Save raw data\n",
    "#     # data.to_excel(\"raw_hvac_data.xlsx\", index=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to fetch data: {e}\")\n",
    "#     traceback.print_exc()  # Print full traceback for SQL errors\n",
    "#     exit()\n",
    "data=pd.read_excel(\"Hvac_data.xlsx\")\n",
    "print(\"Initial data shape:\", data.shape)\n",
    "print(\"Initial columns:\", data.columns.tolist())\n",
    "print(\"Initial data types:\\n\", data.dtypes)  # Check initial types\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "print(\"\\nStarting Data Preprocessing...\")\n",
    "# Rename inconsistent names\n",
    "if 'Name' in data.columns:\n",
    "    data['Name'] = data['Name'].replace('AHU1.1', 'AHU.1')\n",
    "    # Filter for relevant names (Still useful to filter rows, even if 'Name' isn't used as a feature)\n",
    "    data = data[data['Name'].str.contains('HVAC', na=False)]\n",
    "    print(f\"Shape after filtering Name: {data.shape}\")\n",
    "else:\n",
    "    print(\"Warning: 'Name' column not found for filtering.\")\n",
    "\n",
    "\n",
    "# Filter target variable range\n",
    "if 'Active_Energy_Delivered' in data.columns:\n",
    "    # Ensure target is numeric before filtering\n",
    "    data['Active_Energy_Delivered'] = pd.to_numeric(\n",
    "        data['Active_Energy_Delivered'], errors='coerce')\n",
    "    # Drop rows where conversion failed\n",
    "    data.dropna(subset=['Active_Energy_Delivered'], inplace=True)\n",
    "    data = data[(data['Active_Energy_Delivered'] >= 0) &\n",
    "                (data['Active_Energy_Delivered'] < 120)]  # Adjust upper limit if needed\n",
    "    print(f\"Shape after filtering Active_Energy_Delivered: {data.shape}\")\n",
    "else:\n",
    "    print(\"Error: Target column 'Active_Energy_Delivered' not found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Convert Date and handle errors\n",
    "if 'Date' in data.columns:\n",
    "    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "    initial_rows = data.shape[0]\n",
    "    data = data.dropna(subset=['Date'])\n",
    "    print(\n",
    "        f\"Dropped {initial_rows - data.shape[0]} rows due to invalid Date format.\")\n",
    "    print(f\"Shape after handling Date: {data.shape}\")\n",
    "    data = data.sort_values(by='Date')\n",
    "else:\n",
    "    print(\"Error: 'Date' column not found for train/test split. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Impute Jumbo Temperature and Humidity (Check column existence first)\n",
    "temp_cols_exist = all(col in data.columns for col in [\n",
    "                      'Jumbo_Temp1', 'Jumbo_Temp2', 'Jumbo_Temp3'])\n",
    "humidity_cols_exist = all(col in data.columns for col in [\n",
    "                          'Jumbo_Humidity', 'Jumbo_Humidity3'])\n",
    "\n",
    "if temp_cols_exist:\n",
    "    print(\"Imputing Jumbo Temperatures...\")\n",
    "    # Convert to numeric first\n",
    "    for col in ['Jumbo_Temp1', 'Jumbo_Temp2', 'Jumbo_Temp3']:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    data['Jumbo_Temp1'] = data['Jumbo_Temp1'].fillna(\n",
    "        data['Jumbo_Temp2']).fillna(data['Jumbo_Temp3'])\n",
    "else:\n",
    "    print(\"Warning: One or more Jumbo Temperature columns missing, skipping imputation.\")\n",
    "\n",
    "if humidity_cols_exist:\n",
    "    print(\"Imputing Jumbo Humidity...\")\n",
    "    # Convert to numeric first\n",
    "    for col in ['Jumbo_Humidity', 'Jumbo_Humidity3']:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    data['Jumbo_Humidity'] = data['Jumbo_Humidity'].fillna(\n",
    "        data['Jumbo_Humidity3'])\n",
    "else:\n",
    "    print(\"Warning: One or more Jumbo Humidity columns missing, skipping imputation.\")\n",
    "\n",
    "# Drop redundant columns if they exist AND imputation was attempted\n",
    "cols_to_drop = []\n",
    "if temp_cols_exist:\n",
    "    cols_to_drop.extend(['Jumbo_Temp2', 'Jumbo_Temp3'])\n",
    "if humidity_cols_exist:\n",
    "    cols_to_drop.append('Jumbo_Humidity3')\n",
    "\n",
    "data.drop(\n",
    "    columns=[col for col in cols_to_drop if col in data.columns], inplace=True)\n",
    "print(\n",
    "    f\"Dropped redundant columns: {[col for col in cols_to_drop if col in data.columns]}\")\n",
    "\n",
    "\n",
    "# Forward/Backward fill remaining NaNs for imputed columns\n",
    "if 'Jumbo_Temp1' in data.columns:\n",
    "    initial_nan = data['Jumbo_Temp1'].isna().sum()\n",
    "    data['Jumbo_Temp1'] = data['Jumbo_Temp1'].ffill().bfill()\n",
    "    print(\n",
    "        f\"Filled {initial_nan - data['Jumbo_Temp1'].isna().sum()} NaNs in Jumbo_Temp1.\")\n",
    "if 'Jumbo_Humidity' in data.columns:\n",
    "    initial_nan = data['Jumbo_Humidity'].isna().sum()\n",
    "    data['Jumbo_Humidity'] = data['Jumbo_Humidity'].ffill().bfill()\n",
    "    print(\n",
    "        f\"Filled {initial_nan - data['Jumbo_Humidity'].isna().sum()} NaNs in Jumbo_Humidity.\")\n",
    "\n",
    "# Filter based on T2M and Operating_Hours (Check column existence and convert to numeric)\n",
    "if 'T2M' in data.columns:\n",
    "    data['T2M'] = pd.to_numeric(data['T2M'], errors='coerce')\n",
    "    data.dropna(subset=['T2M'], inplace=True)\n",
    "    # Adjusted plausible range\n",
    "    data = data[(data['T2M'] > 0) & (data['T2M'] < 50)]\n",
    "    print(f\"Shape after filtering T2M: {data.shape}\")\n",
    "else:\n",
    "    print(\"Warning: 'T2M' column not found for filtering.\")\n",
    "\n",
    "if 'Operating_Hours' in data.columns:\n",
    "    data['Operating_Hours'] = pd.to_numeric(\n",
    "        data['Operating_Hours'], errors='coerce')\n",
    "    data.dropna(subset=['Operating_Hours'], inplace=True)\n",
    "    # Allow zero operating hours, but filter extremes\n",
    "    data = data[(data['Operating_Hours'] >= 0) &\n",
    "                (data['Operating_Hours'] < 1.1)]  # Keep upper bound reasonable\n",
    "    print(f\"Shape after filtering Operating_Hours: {data.shape}\")\n",
    "else:\n",
    "    print(\"Warning: 'Operating_Hours' column not found for filtering.\")\n",
    "\n",
    "\n",
    "# Create Compressor_delta (Check column existence)\n",
    "if 'Avg_Return_Water_Temp' in data.columns and 'Avg_Supply_water_Temp' in data.columns:\n",
    "    # Ensure columns are numeric before subtraction\n",
    "    data['Avg_Return_Water_Temp'] = pd.to_numeric(\n",
    "        data['Avg_Return_Water_Temp'], errors='coerce')\n",
    "    data['Avg_Supply_water_Temp'] = pd.to_numeric(\n",
    "        data['Avg_Supply_water_Temp'], errors='coerce')\n",
    "    # Only calculate delta if both are non-null\n",
    "    mask = data['Avg_Return_Water_Temp'].notna(\n",
    "    ) & data['Avg_Supply_water_Temp'].notna()\n",
    "    data.loc[mask, 'Compressor_delta'] = data.loc[mask, 'Avg_Return_Water_Temp'] - \\\n",
    "        data.loc[mask, 'Avg_Supply_water_Temp']\n",
    "    print(\"Compressor_delta created for valid rows.\")\n",
    "    # Optionally fill remaining NaNs in Compressor_delta if needed, e.g., with 0 or median\n",
    "    # data['Compressor_delta'].fillna(0, inplace=True)\n",
    "else:\n",
    "    print(\"Warning: Water temperature columns missing, cannot create Compressor_delta.\")\n",
    "\n",
    "\n",
    "# Select final features - INCLUDING 'Name' initially for filtering rows if needed,\n",
    "# but it will be EXCLUDED from X_train/X_test later.\n",
    "# Make sure all desired numerical features are included here\n",
    "final_columns_base = [\n",
    "    'Name', 'T2M', 'Operating_Hours', 'Active_Energy_Delivered',\n",
    "    'Year', 'Month', 'Day', 'hour', 'Date',  # Keep Date for splitting\n",
    "    'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp',\n",
    "    # Include 3rd_Shift if relevant\n",
    "    'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift',\n",
    "    'common', 'General',\n",
    "    # Add other potentially relevant numeric columns if available and preprocessed\n",
    "    'Current_Phase_Average', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line',\n",
    "    'Jumbo_Temp1', 'Jumbo_Humidity'\n",
    "]\n",
    "# Keep only columns that actually exist in the dataframe after preprocessing\n",
    "available_final_columns = [\n",
    "    col for col in final_columns_base if col in data.columns]\n",
    "print(\n",
    "    f\"\\nUsing available columns for final selection: {available_final_columns}\")\n",
    "\n",
    "# Filter the dataframe to only these columns\n",
    "# Use .copy() to avoid SettingWithCopyWarning\n",
    "data = data[available_final_columns].copy()\n",
    "\n",
    "# --- Final Check for NaNs before Split ---\n",
    "print(\"\\nChecking for NaNs before train/test split:\")\n",
    "nan_counts = data.isna().sum()\n",
    "print(nan_counts[nan_counts > 0])\n",
    "# Decide on a strategy for remaining NaNs (e.g., fill with 0, median, mean, or drop rows)\n",
    "# Filling with 0 is simple but might not be optimal for all features.\n",
    "# Consider feature-specific imputation if large numbers of NaNs remain.\n",
    "print(\"Filling remaining NaNs with 0 before split (consider if this is appropriate for all columns)...\")\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessing completed.\")\n",
    "print(\"Final data shape before split:\", data.shape)\n",
    "print(\"Final columns before split:\", data.columns.tolist())\n",
    "print(\"Final data types before split:\\n\", data.dtypes)\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "if data.empty or 'Date' not in data.columns:\n",
    "    print(\"Data is empty or 'Date' column missing after preprocessing. Cannot split.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure 'Date' is still datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "if data['Date'].isna().any():\n",
    "    print(\"Warning: NaNs found in 'Date' column even after preprocessing checks. Dropping affected rows.\")\n",
    "    data.dropna(subset=['Date'], inplace=True)\n",
    "\n",
    "if data.empty:\n",
    "    print(\"Data became empty after final Date check. Cannot split.\")\n",
    "    exit()\n",
    "\n",
    "split_days = 30\n",
    "print(f\"\\nSplitting data: Last {split_days} days for testing.\")\n",
    "test_cutoff_date = data['Date'].max() - pd.Timedelta(days=split_days)\n",
    "train_data = data[data['Date'] <= test_cutoff_date].copy()\n",
    "test_data = data[data['Date'] > test_cutoff_date].copy()\n",
    "\n",
    "print(\n",
    "    f\"Train data range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
    "print(\n",
    "    f\"Test data range: {test_data['Date'].min()} to {test_data['Date'].max()}\")\n",
    "\n",
    "\n",
    "if train_data.shape[0] == 0:\n",
    "    raise ValueError(\n",
    "        \"Training data is empty after split. Check date range and filtering.\")\n",
    "if test_data.shape[0] == 0:\n",
    "    raise ValueError(\n",
    "        \"Test data is empty after split. Check date range and filtering.\")\n",
    "\n",
    "# Handle potential NaNs AGAIN after split (although we filled earlier, good practice)\n",
    "# Using fillna(0) as per the original code's logic after split.\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Drop Date column (must be done AFTER split and potential usage)\n",
    "train_data.drop(columns=['Date'], axis=1, inplace=True)\n",
    "test_data.drop(columns=['Date'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Reset index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nTrain-Test Split completed.\")\n",
    "print(\"Training set shape before dropping non-numerics:\", train_data.shape)\n",
    "print(\"Testing set shape before dropping non-numerics:\", test_data.shape)\n",
    "\n",
    "# --- Model Training and Evaluation (NUMERICAL FEATURES ONLY) ---\n",
    "if not train_data.empty and 'Active_Energy_Delivered' in train_data.columns:\n",
    "    print(\"\\nStarting Model Training (NUMERICAL FEATURES ONLY)...\")\n",
    "\n",
    "    # Prepare data for modeling - Select ONLY numerical features EXPLICITLY\n",
    "    target = 'Active_Energy_Delivered'\n",
    "\n",
    "    # Identify numerical columns from train_data, excluding the target\n",
    "    numerical_cols = train_data.select_dtypes(\n",
    "        include=np.number).columns.tolist()\n",
    "    if target in numerical_cols:\n",
    "        numerical_cols.remove(target)  # Don't include target in features\n",
    "\n",
    "    # Identify categorical columns to EXCLUDE\n",
    "    categorical_cols = train_data.select_dtypes(\n",
    "        include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # --- DEBUG PRINT 1 ---\n",
    "    print(\n",
    "        f\"\\nDEBUG: Numerical columns selected for training: {numerical_cols}\")\n",
    "    # Manually check if 'Year', 'Month', 'Day', 'hour' are in this list.\n",
    "\n",
    "    print(f\"Using ONLY Numerical columns as features: {numerical_cols}\")\n",
    "    print(f\"Excluding Categorical columns: {categorical_cols}\")  # Like 'Name'\n",
    "    print(f\"Target column: {target}\")\n",
    "\n",
    "    # Ensure only existing numerical columns are used (redundant if selection is correct, but safe)\n",
    "    numerical_cols = [\n",
    "        col for col in numerical_cols if col in train_data.columns]\n",
    "\n",
    "    X_train = train_data[numerical_cols]  # Select only numerical features\n",
    "    y_train = train_data[target]\n",
    "\n",
    "    # Ensure test set uses the same numerical columns IN THE SAME ORDER\n",
    "    X_test = test_data[[\n",
    "        col for col in numerical_cols if col in test_data.columns]]\n",
    "    # Check if any columns are missing in test set that were in train set\n",
    "    missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "    if missing_in_test:\n",
    "        print(\n",
    "            f\"Warning: Columns {missing_in_test} present in training features but missing in test features. This might cause errors.\")\n",
    "        # Decide how to handle: drop from X_train, add dummy to X_test, or error out.\n",
    "        # For simplicity here, we'll proceed, but be aware of potential issues.\n",
    "        # A more robust solution might involve aligning columns after the split.\n",
    "\n",
    "    # Ensure target exists in test data\n",
    "    if target not in test_data.columns:\n",
    "        raise ValueError(\n",
    "            f\"Target column '{target}' not found in test_data after preprocessing.\")\n",
    "    y_test = test_data[target]\n",
    "\n",
    "    # --- DEBUG PRINT 2 ---\n",
    "    print(\n",
    "        f\"\\nDEBUG: X_train columns used for fitting: {X_train.columns.tolist()}\")\n",
    "    # Check again if time features are present here.\n",
    "    print(f\"DEBUG: X_train shape: {X_train.shape}\")\n",
    "    print(\n",
    "        f\"DEBUG: X_test columns before processing: {X_test.columns.tolist()}\")\n",
    "    print(f\"DEBUG: X_test shape: {X_test.shape}\")\n",
    "\n",
    "    # Check if numerical columns list is empty or X_train is empty\n",
    "    if not numerical_cols:\n",
    "        raise ValueError(\n",
    "            \"No numerical features identified for training. Check data types and selection.\")\n",
    "    if X_train.empty:\n",
    "        raise ValueError(\n",
    "            \"X_train is empty after selecting numerical features. Check filtering steps.\")\n",
    "    if X_test.empty:\n",
    "        raise ValueError(\n",
    "            \"X_test is empty after selecting numerical features. Check filtering steps and split.\")\n",
    "\n",
    "    # --- Preprocessing Pipeline (Minimal for Numerical Only) ---\n",
    "    # Using ColumnTransformer just to maintain pipeline structure, but only applying 'passthrough'.\n",
    "    # This ensures column order consistency if more complex steps were added later.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Pass through all numerical columns provided\n",
    "            ('num', 'passthrough', numerical_cols)\n",
    "        ],\n",
    "        # Drop any columns not explicitly passed (shouldn't be needed here)\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    # Create and train model pipeline\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),  # Ensures correct column handling\n",
    "        # --- REGULARIZATION ADDED TO PREVENT OVERFITTING ---\n",
    "        ('regressor', ExtraTreesRegressor(\n",
    "            random_state=123,\n",
    "            n_estimators=100,      # Number of trees in the forest\n",
    "            n_jobs=-1,             # Use all available CPU cores\n",
    "            # --- Parameters added to prevent overfitting ---\n",
    "            # Limit how deep each tree can grow (tune this)\n",
    "            max_depth=15,\n",
    "            # Require at least 5 samples in each leaf node (tune this)\n",
    "            min_samples_leaf=5,\n",
    "            # Require at least 10 samples to split a node (tune this)\n",
    "            min_samples_split=10,\n",
    "            # Consider adding max_features (e.g., 'sqrt' or a float < 1.0)\n",
    "            # max_features='sqrt',\n",
    "            oob_score=False  # Can set to True to estimate generalization error without test set\n",
    "        ))\n",
    "        # --- End of Regularization Parameters ---\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "\n",
    "        print(\"\\nFitting the model pipeline...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Model training finished.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        print(\"X_train columns:\", X_train.columns.tolist())\n",
    "        print(\"numerical_cols for preprocessor:\", numerical_cols)\n",
    "        print(\"X_train data types:\\n\", X_train.dtypes)\n",
    "        print(\"Check for non-numeric data or NaNs that might have slipped through.\")\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    train_predictions = model.predict(X_train)\n",
    "    # Use the same columns for prediction as used in training\n",
    "    test_predictions = model.predict(\n",
    "        X_test[X_train.columns])  # Ensure column order\n",
    "\n",
    "    # --- Evaluation Metrics ---\n",
    "    # Define the evaluation function\n",
    "\n",
    "    def evaluate_metrics(y_true, y_pred, dataset_name):\n",
    "        # Ensure inputs are numpy arrays and not empty\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        if y_true.size == 0 or y_pred.size == 0:\n",
    "            print(\n",
    "                f\"Warning: Empty arrays received for metric calculation in {dataset_name}.\")\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        if y_true.size != y_pred.size:\n",
    "            print(\n",
    "                f\"Warning: Mismatched sizes for metric calculation in {dataset_name}. True: {y_true.size}, Pred: {y_pred.size}\")\n",
    "            # Attempt to align if shapes allow, otherwise return NaN\n",
    "            min_size = min(y_true.size, y_pred.size)\n",
    "            y_true = y_true[:min_size]\n",
    "            y_pred = y_pred[:min_size]\n",
    "            if min_size == 0:\n",
    "                return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        # Handle case where variance of y_true is zero\n",
    "        if np.var(y_true) < 1e-9:  # Check for near-zero variance\n",
    "            # R2 is NaN if variance is zero but error exists\n",
    "            r2 = np.nan if mse > 1e-9 else 1.0\n",
    "        else:\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(\n",
    "            f\"\\nEvaluation Metrics for {dataset_name} (Numerical Features Only Model):\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"R-squared (R2) Score: {r2:.4f}\")\n",
    "        return mse, rmse, mae, r2\n",
    "\n",
    "    # Call the evaluation function\n",
    "    train_metrics = evaluate_metrics(\n",
    "        y_train, train_predictions, \"Training Set\")\n",
    "    test_metrics = evaluate_metrics(y_test, test_predictions, \"Test Set\")\n",
    "    print(\"\\nNote: Model accuracy might be lower as categorical features ('Name') were excluded.\")\n",
    "    print(\"Note: Training R2 score should ideally be < 1.0 due to regularization.\")\n",
    "\n",
    "    # --- Feature Importance Analysis (Global - Numerical Only) ---\n",
    "    print(\"\\nCalculating Global Feature Importance (Numerical Features Only)...\")\n",
    "    try:\n",
    "        # Feature names are simply the numerical columns used for training\n",
    "        # Use the list of columns actually present in X_train after selection\n",
    "        feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Get importances - directly corresponds to numerical_cols used\n",
    "        importances = model.named_steps['regressor'].feature_importances_\n",
    "\n",
    "        # Create DataFrame\n",
    "        if len(feature_names) == len(importances):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "            print(\"\\nGlobal Feature Importance (Top 10 Numerical):\")\n",
    "            print(importance_df.head(10))\n",
    "\n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            num_features_to_plot = min(20, importance_df.shape[0])\n",
    "            if num_features_to_plot > 0:\n",
    "                sns.barplot(x='Importance', y='Feature',\n",
    "                            data=importance_df.head(num_features_to_plot))\n",
    "                plt.title(\n",
    "                    'Top Numerical Global Feature Importance (Model using Numerical Only)')\n",
    "                plt.xlabel('Importance Score (e.g., Gini importance)')\n",
    "                plt.ylabel('Numerical Feature')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('numerical_only_model_global_importance.png')\n",
    "                plt.close()\n",
    "                print(\"Numerical-only model global feature importance plot saved.\")\n",
    "                print(\n",
    "                    \">> Check 'numerical_only_model_global_importance.png' to see if Year/Month/Day/Hour appear.\")\n",
    "            else:\n",
    "                print(\"No numerical features found to plot importance.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Error: Mismatch between feature names ({len(feature_names)}) and importances ({len(importances)}).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"\\nCould not calculate or plot global feature importance: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # --- Correlation Analysis ---\n",
    "    # Remains valid as it uses numerical columns\n",
    "    print(\"\\nCalculating Correlation Matrix...\")\n",
    "    # Ensure target is included and use only selected numericals\n",
    "    numeric_train_data_corr = train_data[numerical_cols + [target]].copy()\n",
    "    # Ensure all columns are numeric for correlation\n",
    "    for col in numeric_train_data_corr.columns:\n",
    "        numeric_train_data_corr[col] = pd.to_numeric(\n",
    "            numeric_train_data_corr[col], errors='coerce')\n",
    "    # Drop rows where conversion failed\n",
    "    numeric_train_data_corr.dropna(inplace=True)\n",
    "\n",
    "    if target in numeric_train_data_corr.columns and not numeric_train_data_corr.empty:\n",
    "        correlation_matrix = numeric_train_data_corr.corr()\n",
    "        if target in correlation_matrix.columns:\n",
    "            consumption_correlation = correlation_matrix[target].sort_values(\n",
    "                ascending=False)\n",
    "            print(\"\\nCorrelation with Active Energy Delivered (Numerical Features):\")\n",
    "            print(consumption_correlation)\n",
    "\n",
    "            plt.figure(figsize=(14, 12))  # Increased size for more features\n",
    "            sns.heatmap(correlation_matrix, annot=True,\n",
    "                        cmap='coolwarm', fmt='.2f', annot_kws={'size': 8})\n",
    "            plt.title('Correlation Heatmap of Numerical Features Used in Model')\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "            plt.yticks(rotation=0, fontsize=9)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('numerical_only_model_correlation_heatmap.png')\n",
    "            plt.close()\n",
    "            print(\"Numerical-only model correlation heatmap saved.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\n'{target}' column disappeared from correlation matrix after calculations.\")\n",
    "    else:\n",
    "        print(f\"\\n'{target}' not found or data empty for correlation analysis.\")\n",
    "\n",
    "    # --- Trend Analysis ---\n",
    "    # Remains valid as it uses original columns before feature selection\n",
    "    print(\"\\nPerforming Trend Analysis (Based on training data)...\")\n",
    "    # Use the original train_data before dropping columns for trends\n",
    "    # Get original rows for train\n",
    "    trend_train_data = data[data.index.isin(train_data.index)].copy()\n",
    "\n",
    "    if 'Month' in trend_train_data.columns and 'hour' in trend_train_data.columns:\n",
    "        # Monthly Trend Plot\n",
    "        try:\n",
    "            monthly_consumption = trend_train_data.groupby(\n",
    "                'Month')['Active_Energy_Delivered'].mean()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            monthly_consumption.plot(kind='bar', color='skyblue')\n",
    "            plt.title('Average Monthly Energy Consumption (Training Data)')\n",
    "            plt.xlabel('Month')\n",
    "            plt.ylabel('Avg Active Energy Delivered')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('monthly_consumption_trend.png')\n",
    "            plt.close()\n",
    "            print(\"Monthly consumption trend plot saved as monthly_consumption_trend.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot monthly trend: {e}\")\n",
    "\n",
    "        # Hourly Trend Plot\n",
    "        try:\n",
    "            hourly_consumption = trend_train_data.groupby(\n",
    "                'hour')['Active_Energy_Delivered'].mean().sort_index()  # Sort by hour\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            hourly_consumption.plot(kind='bar', color='salmon')\n",
    "            plt.title('Average Hourly Energy Consumption (Training Data)')\n",
    "            plt.xlabel('Hour of Day (0-23)')\n",
    "            plt.ylabel('Avg Active Energy Delivered')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('hourly_consumption_trend.png')\n",
    "            plt.close()\n",
    "            print(\"Hourly consumption trend plot saved as hourly_consumption_trend.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot hourly trend: {e}\")\n",
    "    else:\n",
    "        print(\"Month or hour column not available in original data for trend analysis.\")\n",
    "\n",
    "    # --- SHAP Analysis (Numerical Features Only) ---\n",
    "    print(\"\\nCalculating SHAP values for Test Set (Numerical Features Only Model)...\")\n",
    "    try:\n",
    "        # 1. Get regressor (preprocessor step is minimal now)\n",
    "        regressor = model.named_steps['regressor']\n",
    "        # Keep for consistency if needed\n",
    "        # Renamed to avoid conflict\n",
    "        preprocessor_step = model.named_steps['preprocessor']\n",
    "\n",
    "        # 2. Transform test data using the fitted preprocessor\n",
    "        # Ensure X_test has the same columns as X_train used for fitting the preprocessor\n",
    "        X_test_aligned = X_test[X_train.columns]  # Align columns explicitly\n",
    "        X_test_processed = preprocessor_step.transform(X_test_aligned)\n",
    "        # Check if sparse matrix and convert if needed (passthrough shouldn't make it sparse, but good practice)\n",
    "        if hasattr(X_test_processed, \"toarray\"):\n",
    "            print(\"Processed test data is sparse, converting to dense array for SHAP.\")\n",
    "            X_test_processed = X_test_processed.toarray()\n",
    "\n",
    "        # 3. Feature names are just the numerical columns used for training/preprocessing\n",
    "        processed_feature_names = X_train.columns.tolist()  # Use columns from X_train\n",
    "\n",
    "        # --- DEBUG PRINT 3 ---\n",
    "        print(\n",
    "            f\"\\nDEBUG: Feature names for SHAP interpretation: {processed_feature_names}\")\n",
    "        print(\n",
    "            f\"DEBUG: Shape of processed test data for SHAP: {X_test_processed.shape}\")\n",
    "        # Ensure the number of columns in X_test_processed matches len(processed_feature_names)\n",
    "\n",
    "        # 4. Create SHAP Explainer (TreeExplainer is suitable for ExtraTrees)\n",
    "        print(\"Initializing SHAP TreeExplainer...\")\n",
    "        explainer = shap.TreeExplainer(regressor)\n",
    "\n",
    "        # 5. Calculate SHAP values (only for numerical features)\n",
    "        print(\"Calculating SHAP values (this may take some time)...\")\n",
    "        # Check expected value if needed: print(explainer.expected_value)\n",
    "        shap_values = explainer.shap_values(\n",
    "            X_test_processed)  # Use processed data\n",
    "        # Should be (n_samples, n_features)\n",
    "        print(f\"SHAP values calculated. Shape: {shap_values.shape}\")\n",
    "\n",
    "        # Check if SHAP values calculation succeeded\n",
    "        if shap_values is None or not isinstance(shap_values, np.ndarray) or shap_values.shape[1] != len(processed_feature_names):\n",
    "            raise ValueError(\n",
    "                f\"SHAP calculation failed or returned unexpected shape. Expected features: {len(processed_feature_names)}, Got shape: {shap_values.shape if shap_values is not None else 'None'}\")\n",
    "\n",
    "        # 6. Find most impactful numerical feature for each prediction\n",
    "        most_impactful_features = []\n",
    "        shap_values_abs = np.abs(shap_values)\n",
    "        for i in range(shap_values_abs.shape[0]):\n",
    "            # Ensure index is within bounds\n",
    "            if shap_values_abs.shape[1] > 0:\n",
    "                max_impact_index = np.argmax(shap_values_abs[i, :])\n",
    "                # Double check index validity\n",
    "                if max_impact_index < len(processed_feature_names):\n",
    "                    most_impactful_feature_name = processed_feature_names[max_impact_index]\n",
    "                    most_impactful_features.append(most_impactful_feature_name)\n",
    "                else:\n",
    "                    # Handle unexpected index case (should not happen if shapes match)\n",
    "                    print(\n",
    "                        f\"Warning: Invalid max_impact_index {max_impact_index} for row {i}. Max possible index: {len(processed_feature_names)-1}. Appending None.\")\n",
    "                    most_impactful_features.append(None)\n",
    "            else:\n",
    "                # Handle case with no features (should not happen here)\n",
    "                most_impactful_features.append(None)\n",
    "\n",
    "        # 7. Add results to test dataframe (use original test_data for context)\n",
    "        # Ensure test_results_df has the same index as y_test, test_predictions etc.\n",
    "        # Use test_data which still contains original columns before numerical selection\n",
    "        test_results_df = test_data.copy()  # Contains original features + target\n",
    "\n",
    "        # Align lengths before adding columns - use index alignment where possible\n",
    "        # Create series with potentially aligned index\n",
    "        y_test_series = pd.Series(\n",
    "            y_test.values, index=test_results_df.index[:len(y_test)])\n",
    "        pred_series = pd.Series(\n",
    "            test_predictions, index=test_results_df.index[:len(test_predictions)])\n",
    "        impact_series = pd.Series(\n",
    "            most_impactful_features, index=test_results_df.index[:len(most_impactful_features)])\n",
    "\n",
    "        test_results_df['Actual_Energy'] = y_test_series\n",
    "        test_results_df['Predicted_Energy'] = pred_series\n",
    "        test_results_df['Most_Impactful_Num_Feature'] = impact_series\n",
    "\n",
    "        # Calculate error safely after adding predictions\n",
    "        if 'Actual_Energy' in test_results_df and 'Predicted_Energy' in test_results_df:\n",
    "            test_results_df['Prediction_Error'] = test_results_df['Predicted_Energy'] - \\\n",
    "                test_results_df['Actual_Energy']\n",
    "        else:\n",
    "            print(\n",
    "                \"Warning: Could not calculate Prediction_Error due to missing Actual/Predicted columns.\")\n",
    "\n",
    "        # 8. Add SHAP values (only for numerical features)\n",
    "        # Ensure shap_values rows match test_results_df index\n",
    "        if shap_values.shape[0] == len(test_results_df):\n",
    "            shap_df = pd.DataFrame(shap_values, columns=[\n",
    "                                   f\"SHAP_{name}\" for name in processed_feature_names], index=test_results_df.index)\n",
    "            test_results_with_shap = pd.concat(\n",
    "                [test_results_df, shap_df], axis=1)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Length mismatch: SHAP values ({shap_values.shape[0]}) vs test_results_df ({len(test_results_df)}). SHAP values will not be added to the output CSV.\")\n",
    "            # Continue without SHAP values if mismatch\n",
    "            test_results_with_shap = test_results_df  # Assign the df without shap cols\n",
    "\n",
    "        # 9. Save detailed results\n",
    "        output_filename = 'Test_Predictions_Numerical_Only_SHAP_new.csv'\n",
    "        print(\n",
    "            f\"\\nSaving detailed test predictions with SHAP info to '{output_filename}'...\")\n",
    "        test_results_with_shap.to_csv(output_filename, index=False)\n",
    "        print(f\"Test predictions saved.\")\n",
    "\n",
    "        # 10. Visualize SHAP Summary (Numerical Features Only)\n",
    "        # Check if SHAP values were successfully calculated and added\n",
    "        if 'shap_df' in locals() and shap_values is not None and shap_values.shape[0] > 0:\n",
    "            print(\"Generating SHAP summary plot...\")\n",
    "            plt.figure()  # Create a new figure context\n",
    "            # Use X_test_processed (the actual data SHAP values were calculated on) for coloring\n",
    "            # Pass feature_names explicitly\n",
    "            shap.summary_plot(shap_values, X_test_processed,\n",
    "                              feature_names=processed_feature_names,\n",
    "                              max_display=20, show=False)\n",
    "            plt.title('SHAP Summary Plot (Numerical Features Only Model)')\n",
    "            # Add explanation for the plot\n",
    "            plt.xlabel(\"SHAP value (impact on model output)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('numerical_only_model_shap_summary.png')\n",
    "            plt.close()\n",
    "            print(\"Numerical-only model SHAP summary plot saved.\")\n",
    "            print(\">> Check 'numerical_only_model_shap_summary.png' - this shows the overall impact distribution for each feature.\")\n",
    "            print(\n",
    "                \">> Features higher up are more important overall. Check if Year/Month/Day/Hour appear here.\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Skipping SHAP summary plot generation due to issues in SHAP calculation or length mismatch.\")\n",
    "\n",
    "        # 11. Analyze frequency of most impactful numerical features\n",
    "        if 'Most_Impactful_Num_Feature' in test_results_df:\n",
    "            # Drop potential None values before counting\n",
    "            impact_series_clean = test_results_df['Most_Impactful_Num_Feature'].dropna(\n",
    "            )\n",
    "            if not impact_series_clean.empty:\n",
    "                impact_summary = impact_series_clean.value_counts().reset_index()\n",
    "                impact_summary.columns = ['Feature', 'Times_Most_Impactful']\n",
    "                print(\"\\nFrequency of numerical features being most impactful (Top 20):\")\n",
    "                print(impact_summary.head(20))\n",
    "\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.barplot(x='Times_Most_Impactful', y='Feature',\n",
    "                            data=impact_summary.head(20), palette='viridis')  # Added palette\n",
    "                plt.title(\n",
    "                    'Top 20 Most Frequently Impactful Numerical Features (SHAP)')\n",
    "                plt.xlabel(\n",
    "                    'Number of Times Feature had Highest SHAP Value in Test Set')\n",
    "                plt.ylabel('Numerical Feature')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('numerical_only_model_impact_frequency.png')\n",
    "                plt.close()\n",
    "                print(\n",
    "                    \"Numerical-only model most impactful feature frequency plot saved.\")\n",
    "                print(\n",
    "                    \">> Check 'numerical_only_model_impact_frequency.png'. If Year/Month/Day/Hour are missing or low,\")\n",
    "                print(\n",
    "                    \">> it means other features usually had a higher individual impact on predictions.\")\n",
    "            else:\n",
    "                print(\n",
    "                    \"No impactful features found to summarize frequency (column might be all None).\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Could not analyze impact frequency: 'Most_Impactful_Num_Feature' column missing.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\nSHAP library not found. Please install it: pip install shap\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not complete SHAP analysis: {str(e)}\")\n",
    "        print(\"Check shapes and consistency between X_test_processed, shap_values, and processed_feature_names.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # --- Save General Results ---\n",
    "    print(\"\\nSaving general results (Numerical Only Model)...\")\n",
    "    excel_filename = 'Consumption_Insights_Numerical_Only_new.xlsx'\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_filename) as writer:\n",
    "            # Global Importance\n",
    "            if 'importance_df' in locals() and not importance_df.empty:\n",
    "                importance_df.to_excel(\n",
    "                    writer, sheet_name='Numerical Global Importance', index=False)\n",
    "            else:\n",
    "                print(\"Skipping Global Importance sheet (not calculated or empty).\")\n",
    "\n",
    "            # Correlation\n",
    "            if 'consumption_correlation' in locals() and not consumption_correlation.empty:\n",
    "                consumption_correlation.to_frame().reset_index().rename(columns={'index': 'Feature', target: 'Correlation'}).to_excel(\n",
    "                    writer, sheet_name='Correlation with Target', index=False)\n",
    "            else:\n",
    "                print(\"Skipping Correlation sheet (not calculated or empty).\")\n",
    "\n",
    "            # Metrics\n",
    "            if 'train_metrics' in locals() and 'test_metrics' in locals():\n",
    "                metrics_df = pd.DataFrame({\n",
    "                    'Metric': ['MSE', 'RMSE', 'MAE', 'R2'],\n",
    "                    # Format as string for Excel clarity, handle potential NaNs\n",
    "                    'Training': [f\"{m:.4f}\" if pd.notna(m) else 'N/A' for m in train_metrics],\n",
    "                    'Test': [f\"{m:.4f}\" if pd.notna(m) else 'N/A' for m in test_metrics]\n",
    "                })\n",
    "                metrics_df.to_excel(\n",
    "                    writer, sheet_name='Model Metrics', index=False)\n",
    "            else:\n",
    "                print(\"Skipping Metrics sheet (not calculated).\")\n",
    "\n",
    "            # Test Predictions Summary (basic)\n",
    "            # Ensure the results DataFrame and required columns exist\n",
    "            required_cols = ['Actual_Energy', 'Predicted_Energy',\n",
    "                             'Prediction_Error', 'Most_Impactful_Num_Feature']\n",
    "            if 'test_results_with_shap' in locals() and all(col in test_results_with_shap for col in required_cols):\n",
    "                basic_predictions_df = test_results_with_shap[required_cols].copy(\n",
    "                )\n",
    "                basic_predictions_df.to_excel(\n",
    "                    writer, sheet_name='Test Predictions Summary', index=False)\n",
    "            else:\n",
    "                print(\n",
    "                    \"Could not write Test Predictions Summary to Excel as results dataframe or columns are missing.\")\n",
    "\n",
    "        print(\n",
    "            f\"General insights for numerical-only model saved to '{excel_filename}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Could not save general results to Excel ('{excel_filename}'): {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\nAnalysis completed (Numerical Features Only Model).\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data available for training or 'Active_Energy_Delivered' column missing after preprocessing.\")\n",
    "    print(\"Analysis could not be performed.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (307404, 32)\n",
      "Initial columns: ['Time-Hourly', 'Name', 'Active_Energy_Delivered', 'Operating_Hours', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Humidity3', 'Jumbo_Temp2', 'Jumbo_Temp1', 'Jumbo_Temp3', 'Jumbo_Humidity', 'T2M', 'Date_Time', 'Avg_Return_Water_Temp', 'Avg_Supply_water_Temp', 'TimeStamp', 'Year', 'Month', 'Day', 'hour', 'Date', 'Start Date', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Left_Right_Name', 'Score', 'Right_Date_Time', 'Right_3rd_Shift', 'Result']\n",
      "Initial data types:\n",
      " Time-Hourly                                object\n",
      "Name                                       object\n",
      "Active_Energy_Delivered                   float64\n",
      "Operating_Hours                           float64\n",
      "Average_Voltage_Line_to_Neutral           float64\n",
      "Average_Voltage_Line_to_Line              float64\n",
      "Jumbo_Humidity3                           float64\n",
      "Jumbo_Temp2                               float64\n",
      "Jumbo_Temp1                               float64\n",
      "Jumbo_Temp3                               float64\n",
      "Jumbo_Humidity                            float64\n",
      "T2M                                       float64\n",
      "Date_Time                                 float64\n",
      "Avg_Return_Water_Temp                     float64\n",
      "Avg_Supply_water_Temp                     float64\n",
      "TimeStamp                          datetime64[ns]\n",
      "Year                                        int64\n",
      "Month                                       int64\n",
      "Day                                         int64\n",
      "hour                                        int64\n",
      "Date                                       object\n",
      "Start Date                                 object\n",
      "1st_Shift                                 float64\n",
      "2nd_Shift                                 float64\n",
      "3rd_Shift                                 float64\n",
      "common                                    float64\n",
      "General                                   float64\n",
      "Left_Right_Name                            object\n",
      "Score                                     float64\n",
      "Right_Date_Time                           float64\n",
      "Right_3rd_Shift                           float64\n",
      "Result                                    float64\n",
      "dtype: object\n",
      "\n",
      "Starting Data Preprocessing...\n",
      "Shape after filtering Name: (105645, 32)\n",
      "Shape after filtering Active_Energy_Delivered: (94726, 32)\n",
      "Dropped 0 rows due to invalid Date format.\n",
      "Shape after handling Date: (94726, 32)\n",
      "Imputing Jumbo Temperatures...\n",
      "Imputing Jumbo Humidity...\n",
      "Dropped redundant columns: []\n",
      "Filled 212 NaNs in Jumbo_Temp1.\n",
      "Filled 212 NaNs in Jumbo_Humidity.\n",
      "Shape after filtering T2M: (90263, 29)\n",
      "Shape after filtering Operating_Hours: (90228, 29)\n",
      "Compressor_delta created for valid rows.\n",
      "\n",
      "Using available columns for final selection: ['Name', 'T2M', 'Operating_Hours', 'Active_Energy_Delivered', 'Year', 'Month', 'Day', 'hour', 'Date', 'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity']\n",
      "\n",
      "Checking for NaNs before train/test split:\n",
      "Avg_Supply_water_Temp    83948\n",
      "Avg_Return_Water_Temp    83948\n",
      "Compressor_delta         83948\n",
      "1st_Shift                60812\n",
      "2nd_Shift                59729\n",
      "3rd_Shift                86605\n",
      "common                   87645\n",
      "General                  57317\n",
      "dtype: int64\n",
      "Filling remaining NaNs with 0 before split (consider if this is appropriate for all columns)...\n",
      "\n",
      "Preprocessing completed.\n",
      "Final data shape before split: (90228, 21)\n",
      "Final columns before split: ['Name', 'T2M', 'Operating_Hours', 'Active_Energy_Delivered', 'Year', 'Month', 'Day', 'hour', 'Date', 'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity']\n",
      "Final data types before split:\n",
      " Name                                       object\n",
      "T2M                                       float64\n",
      "Operating_Hours                           float64\n",
      "Active_Energy_Delivered                   float64\n",
      "Year                                        int64\n",
      "Month                                       int64\n",
      "Day                                         int64\n",
      "hour                                        int64\n",
      "Date                               datetime64[ns]\n",
      "Avg_Supply_water_Temp                     float64\n",
      "Avg_Return_Water_Temp                     float64\n",
      "Compressor_delta                          float64\n",
      "1st_Shift                                 float64\n",
      "2nd_Shift                                 float64\n",
      "3rd_Shift                                 float64\n",
      "common                                    float64\n",
      "General                                   float64\n",
      "Average_Voltage_Line_to_Neutral           float64\n",
      "Average_Voltage_Line_to_Line              float64\n",
      "Jumbo_Temp1                               float64\n",
      "Jumbo_Humidity                            float64\n",
      "dtype: object\n",
      "\n",
      "Splitting data: Last 30 days for testing.\n",
      "Train data range: 2023-09-30 00:00:00 to 2025-09-24 00:00:00\n",
      "Test data range: 2025-09-25 00:00:00 to 2025-10-24 00:00:00\n",
      "\n",
      "Train-Test Split completed.\n",
      "Training set shape before dropping non-numerics: (85562, 20)\n",
      "Testing set shape before dropping non-numerics: (4666, 20)\n",
      "\n",
      "Starting Model Training (NUMERICAL FEATURES ONLY)...\n",
      "\n",
      "DEBUG: Numerical columns selected for training: ['T2M', 'Operating_Hours', 'Year', 'Month', 'Day', 'hour', 'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity']\n",
      "Using ONLY Numerical columns as features: ['T2M', 'Operating_Hours', 'Year', 'Month', 'Day', 'hour', 'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity']\n",
      "Excluding Categorical columns: ['Name']\n",
      "Target column: Active_Energy_Delivered\n",
      "\n",
      "DEBUG: X_train columns used for fitting: ['T2M', 'Operating_Hours', 'Year', 'Month', 'Day', 'hour', 'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity']\n",
      "DEBUG: X_train shape: (85562, 18)\n",
      "DEBUG: X_test columns before processing: ['T2M', 'Operating_Hours', 'Year', 'Month', 'Day', 'hour', 'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity']\n",
      "DEBUG: X_test shape: (4666, 18)\n",
      "Shiva_ training columns Index(['T2M', 'Operating_Hours', 'Year', 'Month', 'Day', 'hour',\n",
      "       'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta',\n",
      "       '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General',\n",
      "       'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line',\n",
      "       'Jumbo_Temp1', 'Jumbo_Humidity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:20:28.356384Z",
     "start_time": "2025-10-26T13:20:28.349931Z"
    }
   },
   "cell_type": "code",
   "source": "train_data.columns",
   "id": "db4b97b8c62eda48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'T2M', 'Operating_Hours', 'Active_Energy_Delivered', 'Year',\n",
       "       'Month', 'Day', 'hour', 'Avg_Supply_water_Temp',\n",
       "       'Avg_Return_Water_Temp', 'Compressor_delta', '1st_Shift', '2nd_Shift',\n",
       "       '3rd_Shift', 'common', 'General', 'Average_Voltage_Line_to_Neutral',\n",
       "       'Average_Voltage_Line_to_Line', 'Jumbo_Temp1', 'Jumbo_Humidity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T13:23:01.328413Z",
     "start_time": "2025-10-26T13:23:01.322541Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.columns",
   "id": "e1b92be6937d2022",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T2M', 'Operating_Hours', 'Year', 'Month', 'Day', 'hour',\n",
       "       'Avg_Supply_water_Temp', 'Avg_Return_Water_Temp', 'Compressor_delta',\n",
       "       '1st_Shift', '2nd_Shift', '3rd_Shift', 'common', 'General',\n",
       "       'Average_Voltage_Line_to_Neutral', 'Average_Voltage_Line_to_Line',\n",
       "       'Jumbo_Temp1', 'Jumbo_Humidity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a1d97b450710ae1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
